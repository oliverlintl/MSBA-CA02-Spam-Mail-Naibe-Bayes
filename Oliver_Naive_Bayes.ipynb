{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "y2D_w-8kSmzq"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os   # for reading files\n",
    "import numpy as np    \n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: book : phonetic / speech production\n",
      "\n",
      "shigeru kiritanus , hajime hirose hiroya fujisakus ( editor ) speech production language honor osamu fujimura 1997 . 23 x 15 , 5 cm . x , 302 page . cloth dm 188 , - / approx . us $ 134 . 0 isbn 3-11 - 6847 - 0 speech research 13 mouton de gruyter * berlin * york osamu fujimura renown interest competence wide variety subject rang physics , physiology phonetics linguistics artificial intelligence . through fusion discipline show us human speech language relate physical physiological process phonetics abstract , higher-level linguistic structure . reflect osama fujimura 's long-stand interest , chapter volume provide wide perspective various aspect speech production ( physical , physiological , syntactic , information theoretic ) relationship structure speech language . content 1 background * manfr r . schroeder , speech : physicist remember * 2 larygeal function speech * minoru hirano , kiminorus sato keiichiro yukizane , male - female difference anterior commissure angle o christy l . ludlow , susan e . sedory holzer mihoko fujita , correlation among intrinsic laryngeal muscle during speech gesture * ingo r . titze , regulation fundamental frequency physiologically - base model larynx * shigeru kiritanus seijus niimus , high - speed digital image analysis temporal change vocal fold vibration tremor * masayukus sawashima , phonetic control glottal open * 3 voice source characteristic speech * gunnar fant , frequency domain analysis glottal flow : lf - model revisit * janet pierrehumbert , consequence intonation voice source o noriko umeda , fundamental frequency rule english discourse * hajime hirose , physiological acoustical correlate voice distinction esophageal speech o 4 articulatory organization o morri halle kenneth n . steven , postalveolar fricative polish * thoma h . crystal arthur s . house , note duration american english consonant o shinjus maeda kiyoshus honda , articulatory coordination neurobiological aspect * joseph s . perkell marc h . cohen , token - to-token variation tongue-body vowel target : effect context o ilse lehiste , phonetic realization haiku form estonian poetry , compare japanese * m . mohan sondhus , synthesis code speech physiological model o 5 verbal behavior : sound structure , information structure * john j . ohalum , comparison speech sound : distance v . cost metric * jame d . mccawley , note japanese passive * hiroya fujisakus , sentence production information * index _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ mouton de gruyter walter de gruyter , inc . postfach 30 34 21 200 saw mill river road d-10728 berlin hawthorne , ny 10532 germany usa fax : + 49 ( 0 ) 30 26005-351 fax : + 1 914 747-1326 email : 100064 . 2307 @ compuserve . com publication de gruyter order vium world wide web : http : / / www . degruyter . de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Quickly check what a text file looks like\n",
    "print(open(\"./test-mails\\\\8-899msg1.txt\", 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jW87Q8VGfkQy"
   },
   "source": [
    "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "e8Bduzj2eXLa"
   },
   "outputs": [],
   "source": [
    "def make_Dictionary(root_dir):\n",
    "  all_words=[]    #create empty list\n",
    "  emails = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]     # create a list with email paths \n",
    "  for mail in emails:    # loop through all emails in the list of emails \n",
    "    with open(mail) as m:    # open emails \n",
    "        for i, line in enumerate(m):    # loop through each line\n",
    "            if i == 2:     # start at the 3rd line which is text\n",
    "                words = line.split()    # split the sentence into words \n",
    "                all_words += words    #append each word tolist\n",
    "  dictionary = Counter(all_words)    # Counter count how many time each word appears in a list and create dictionary\n",
    "\n",
    "  list_to_remove = list(dictionary)    # creating a list of keys/words without the count\n",
    "  \n",
    "  for item in list_to_remove:\n",
    "    if item.isalpha() == False:    # .isalpha() check if all characters are letters\n",
    "      del dictionary[item]    # remove all non letter characters\n",
    "    elif len(item) == 1:    # check if any words are one-letter word\n",
    "      del dictionary[item]    # remove all 1 letter words\n",
    "  dictionary = dictionary.most_common(3000)    # .most_common() is a method to the Counter dictionary subclass\n",
    "  return dictionary    # return a list of tuples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj5-KrOdjSbg"
   },
   "source": [
    "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spame or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KVGi89OmjVwF"
   },
   "outputs": [],
   "source": [
    "spam = 'spmsg'\n",
    "def extract_features(mail_dir):\n",
    "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]    # get file path\n",
    "    features_matrix = np.zeros((len(files), 3000))    # creating an empty matrix\n",
    "    train_labels = np.zeros(len(files))\n",
    "    count = 1;  \n",
    "    docID = 0;  \n",
    "    for fil in files:    # for path in file_paths, looping through each files from folder\n",
    "        with open(fil) as fi:\n",
    "            for i, line in enumerate(fi):    # loop through each line\n",
    "                if i == 2:    # start at the 3rd line which is text\n",
    "                    words = line.split()    # split on white space\n",
    "                    for word in words:    # loop through each word in a sentence\n",
    "                        wordID = 0\n",
    "                        for i, d in enumerate(dictionary):    # d is the tuples in the list of tuples\n",
    "                            if d[0] == word:    # d[0] is the word in the tuple, d[1] is the count of that word\n",
    "                                wordID = i    # if there is a match of the word, i would be the column number of the feature_matrix\n",
    "                                features_matrix[docID, wordID] = words.count(word)   # filling the matrix with the count of each word in a mail\n",
    "            train_labels[docID] = 0;  # assume every sentence is non-spam which is labeled by 0\n",
    "# An Alternative\n",
    "#             try :\n",
    "#                 fil.index(spam)\n",
    "#                 train_labels[docID] = 1\n",
    "#             except ValueError:\n",
    "#                 print(\"Not found!\")       \n",
    "            filepathTokens = fil.split('\\\\')    # split the file path in to seperate strings into a list. The string is actually seperated by '\\\\' nor '/'\n",
    "            lastToken = filepathTokens[-1]    # choose the last item\n",
    "            if lastToken.startswith('spmsg'):    # all spam mesages starts with spmsg\n",
    "                train_labels[docID] = 1;    # spam messages are labeled as 1 \n",
    "                count = count + 1    # add 1 to count when there is a match\n",
    "            docID = docID + 1    # move on to the next message and add 1 to docID\n",
    "    return features_matrix, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('order', 1414), ('address', 1293), ('report', 1216), ('mail', 1127), ('send', 1079), ('language', 1072), ('email', 1051), ('program', 1001), ('our', 987), ('list', 935)]\n",
      "reading and processing emails from TRAIN and TEST folders...\n"
     ]
    }
   ],
   "source": [
    "# set path for files\n",
    "TRAIN_DIR = './train-mails'\n",
    "TEST_DIR = './test-mails'\n",
    "\n",
    "# create dictionary\n",
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "print(dictionary[0:10]) # check dictionary\n",
    "\n",
    "print('reading and processing emails from TRAIN and TEST folders...')\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
    "\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model using Gaussian Naive Bayes algorithm......\n",
      "Training completed\n",
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data ... now printing Accuracy Score by comparing predicted labels with the test labels:\n",
      "0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "print('Training Model using Gaussian Naive Bayes algorithm......')\n",
    "model.fit(features_matrix, labels)\n",
    "print('Training completed')\n",
    "print('testing trained model to predict Test Data labels')\n",
    "predicted_labels = model.predict(test_features_matrix)\n",
    "print('Completed classification of the Test Data ... now printing Accuracy Score by comparing predicted labels with the test labels:')\n",
    "print(accuracy_score(test_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
